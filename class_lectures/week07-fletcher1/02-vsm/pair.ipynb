{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Handwritten Digits\n",
    "Let's try using PCA to better explore our familiar handwritten digits dataset.\n",
    "\n",
    "### Load the Data\n",
    "Use a call to [`sklearn.datasets.load_digits()`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) to load the digits dataset into a variable called `digits` with only the digits 0-5 (`n_class=6`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the dataset with 6 classes (digits 0 through 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a `pd.DataFrame()` and `pd.Series()` constructor to convert the feature and target data from `digits` into a dataframe `X` and a series `y` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the explanatory (or independent or feature) variables into a dataframe X\n",
    "\n",
    "# load the target (or dependent or class) variable into a series y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the shape of the data matrix `X` by a call to shape just to make sure the number of samples and features is right.  Store the number of samples in `n_samples` and the number of features in `n_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the number of rows (samples) and columns (features) with a call to shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data\n",
    "Let's take a look at what some of our data actually looks like by printing out a number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_img_per_row = 20 # number of digits per row\n",
    "img = np.zeros((10*n_img_per_row, 10*n_img_per_row)) # generate a new 200x200 array filled with zeros\n",
    "for i in range(n_img_per_row):\n",
    "    ix = 10 * i + 1\n",
    "    for j in range(n_img_per_row):\n",
    "        iy = 10 * j + 1\n",
    "        img[ix:ix+8, iy:iy+8] = X.ix[i*n_img_per_row + j].reshape((8, 8)) # set each 8x8 area of the img to the values of each row (reshaped from 1x64 to 8x8)\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=250) # define a figure, with size (width and height) and resolution\n",
    "#axes(frameon = 0) # remove the frame/border from the axes\n",
    "plt.imshow(img, cmap=plt.cm.binary) # show the image using a binary color map\n",
    "plt.xticks([]) # no x ticks\n",
    "plt.yticks([]) # no y ticks\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Standardization\n",
    "Generally, PCA requires centering the data (i.e., subtracting the mean from each data point for each feature), because otherwise the first component may not truly describe the largest direction of variation in the data, but rather the mean of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# global centering\n",
    "X_centered = X - X.mean()\n",
    "\n",
    "# print again\n",
    "n_img_per_row = 20 # number of digits per row\n",
    "img = np.zeros((10*n_img_per_row, 10*n_img_per_row)) # generate a new 200x200 array filled with zeros\n",
    "for i in range(n_img_per_row):\n",
    "    ix = 10 * i + 1\n",
    "    for j in range(n_img_per_row):\n",
    "        iy = 10 * j + 1\n",
    "        img[ix:ix+8, iy:iy+8] = X_centered.ix[i*n_img_per_row + j].reshape((8, 8)) # set each 8x8 area of the img to the values of each row (reshaped from 1x64 to 8x8)\n",
    "\n",
    "plt.figure(figsize=(8, 8), dpi=250) # define a figure, with size (width and height) and resolution\n",
    "#axes(frameon = 0) # remove the frame/border from the axes\n",
    "plt.imshow(img, cmap=plt.cm.binary) # show the image using a binary color map\n",
    "plt.xticks([]) # no x ticks\n",
    "plt.yticks([]) # no y ticks\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA\n",
    "Now let's generate a PCA on the digits data.  \n",
    "- Create an object called `pca` that is a `sklearn.decomposition.PCA` object with `n_components=64` (we'll want to examine the different components later so keep all of the 64 possible principal components)\n",
    "- Use `pca.fit_transform()` on our centered digit data `X_centered` to transform the digit data into the PCA space.  Store the result in `X_pca`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create PCA-generator\n",
    "\n",
    "# Transform X_centered to X_pca via a fit_transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Results\n",
    "Now that we have a PCA we can examine the different components.  Let's try to make a bar plot of the variance explained (proportion) for each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a figure with figsize=(20,5)\n",
    "\n",
    "# Generate axes via add_subplot\n",
    "\n",
    "# Call ax.bar on the appropriate x and y to generate a bar graph of the explained variance for each component\n",
    "# x should be the numbers 1 to 64\n",
    "# y should be pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "ax.set_title(\"Explained variance\", size=32)\n",
    "ax.set_ylabel(\"Percent explained\", size=24)\n",
    "ax.set_xlabel(\"Principal Component\", size=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions\n",
    "- What do you notice?\n",
    "- How many principal components do you think you might retain?\n",
    "\n",
    "Dimensionality reduction is a one-way transformation that induces a loss of information. We can try to minimize the loss of information while retaining the benefits of dimensionality reduction by trying to find the number of principal components needed to effectively represent the original dataset. This number can often be determined by the \"elbow\" or \"knee\" point, which is considered to be the natural break between the useful principal components (or dimensions) and residual noise. We can find the elbow point by computing PCA on our dataset and observing the number of principal components after which the amount of variance explained displays a natural break or drop-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets look at the first 2 components:\n",
    "Looking at only the first 2 components allows us to make actually visualize our dataset in 2-D to the best level that we can.  Use whatever method you like to determine how much variance is explained by the first 2 components in the digits PCA (**HINT:** use `explained_variance_ratio_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#It's easy to represent 2 elements in a plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're going to do in the next few steps is visualize how the different digits cluster in just 2 dimensions in our 2-D PCA space.  We'll start with a function `plot_embedding` which takes data and plots the digits in the 2-D space where the point is actually rendered as the digit itself.\n",
    "\n",
    "First we'll call this function on data randomly projected into 2-D from the original space and then on the PCA 2-D projection.  You should notice how well the PCA projection separates out the different classes even with just the first 2 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_embedding(X, title=None):\n",
    "    # min-max normalization\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=(10, 6), dpi=250)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.axis('off')\n",
    "    ax.patch.set_visible(False)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(digits.target[i]), color=plt.cm.Set1(y[i] / 10.), fontdict={'weight': 'bold', 'size': 12})\n",
    "\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.ylim([-0.1,1.1])\n",
    "    plt.xlim([-0.1,1.1])\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_embedding(X_pca, \"2 components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA has no information about the classes, but provides insight into the distribution of different numbers in the parameter space\n",
    "- 0 and 4 tend to be more distinct then 1, 2, 5\n",
    "- Does this make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now Cluster!\n",
    "\n",
    "Use KMeans (or you can try others, but KMeans is probably good enough judging from the plot above) to cluster the observations from X_pca into 6 groups.  How do the results look?  Try out keeping different numbers of principal components and see if this changes your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KMeans clustering of digits with PCA vectors"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
