### Plan for Thursday, October 19

#### Overview  

Today you'll see your first look at some magic.  I'll talk about the illustrious **Neural Networks**, walking through a simple example of how they work.  For now, this is probably not a classification technique that you will make use of much on McNulty, as they often don't work so well until you can build a really big one.  But this will set you up for later in the course when we return to Neural Networks for Deep Learning, primarily on image and text processing.

### Schedule

**9:15 am**: Coffee and waking up

**9:30 am**: [Pair Problem](pair.md)

| Student 1 | Student 2 |
|---|---|
| Rebekah | Carl |
| Andre | Michael |
| Sufyan | Mike |
| Yanxi | Chuoran |
| Kenny | Pradnya |
| Laura | Kalgi |
| Jeff | Trent |

**10:15 am**: Intro to Neural Networks (White Board)  
* [Feed-Forward Networks](Intro_NN.pdf)
* [Backpropagation](Backpropagation.pdf)

**11:30 am**: [Linear Algebra for Neural Networks](Neural Networks.ipynb)

**12:00 pm**: Tu Manges!

**1:30 pm**: Investigation Presentation: Trent Watt on "No Free Lunch"

**1:45 pm**: More McNulty!

### Further Resources

 * [Stochastic Gradient Descent Math and Tricks for Success](http://research.microsoft.com/pubs/192769/tricks-2012.pdf)
 * [Stochastic Gradient Descent in scikit](http://scikit-learn.org/stable/modules/sgd.html)
 * [sklearn SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)
 * [Loss functions in sklearn](http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html)
 * Check out the [perceptron](https://github.com/simple-statistics/simple-statistics/blob/master/src/perceptron.js) in [simple-statistics](http://simplestatistics.org/). (And start thinking about JavaScript!)
 * [Easy to follow youtube lecture series on neural networks](https://www.youtube.com/watch?v=bxe2T-V8XRs)
 * [Short intro to perceptrons and neural networks](http://www.cprogramming.com/tutorial/AI/perceptron.html)
 * [Perceptrons](http://page.mi.fu-berlin.de/rojas/neural/chapter/K4.pdf)
 * Great explanations of deep learning on [Olah's blog](http://colah.github.io/)

---

 * [PyBrain](http://pybrain.org/): a modular neural net machine learning library for Python
 * [neurolab example](https://pythonhosted.org/neurolab/ex_newff.html)
 * [pylearn2 multilayer perceptron intro and demonstration](http://nbviewer.ipython.org/github/lisa-lab/pylearn2/blob/master/pylearn2/scripts/tutorials/multilayer_perceptron/multilayer_perceptron.ipynb)
 * [Pylearn2 in practice](http://fastml.com/pylearn2-in-practice/)

---

 * [Intro to Restricted Boltzmann Machines](http://deeplearning.net/tutorial/rbm.html)
 * [Restricted Boltzmann Machines Math](http://image.diku.dk/igel/paper/AItRBM-proof.pdf)
 * See also the [Restricted Boltzmann Machine features for digit classification](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html) example in the `sklearn` documentation.

---

 * [Deep Learning](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html) review paper in Nature by Yann LeCun, Yoshua Bengio & Geoffrey Hinton (oh - we've got a copy of [the PDF](deep_learning_nature_review_2015.pdf))
 * [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/), a free online book
 * [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf), an early paper showing how effective deep convolutional nets could be

---

Deep Learning Frameworks:

 * [Caffe](http://caffe.berkeleyvision.org/)
 * [Theano](http://deeplearning.net/software/theano/) / [Keras](http://keras.io/)
 * [TensorFlow!](https://www.tensorflow.org/tutorials/)

---

Note that [word2vec](https://code.google.com/p/word2vec/) is an application of these ideas!
